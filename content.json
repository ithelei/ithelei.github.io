{"meta":{"title":"养码哥","subtitle":null,"description":null,"author":"He Lei","url":"http://www.ithelei.com","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2019-08-26T13:42:29.302Z","updated":"2019-08-26T05:57:26.706Z","comments":false,"path":"/404.html","permalink":"http://www.ithelei.com//404.html","excerpt":"","text":""},{"title":"关于","date":"2019-09-02T13:33:55.787Z","updated":"2019-09-02T13:33:55.787Z","comments":false,"path":"about/index.html","permalink":"http://www.ithelei.com/about/index.html","excerpt":"","text":"2015年-2016年，主要集中在数字化（教育）领域。 2017.02.21 https://www.jianshu.com/p/2fcfb711db09 2017.03.10 https://www.jianshu.com/p/8f1d8f90ab8b 2017.03.18 https://www.jianshu.com/p/16bce5d15308 2017.03.26 https://www.jianshu.com/p/37865c28091f 2017.05.15（学习微安老师课程） https://www.jianshu.com/p/dfdc042074af 2017.11.11 下定决心研究技术。买书，静下心，扎实学技术。 2019.09.01 截止到目前，我买了近一万多块钱的书籍，有的还没有学。（但是每天都在精进） 说实话，技术在，心里踏实。（只为更好） 以下是我自己的书。 视频https://jishu-resource.oss-cn-beijing.aliyuncs.com/blog-img/b364346a1620a2ac81e1dc64e494274a.mp4 不忘初心，继续前行。"},{"title":"书单","date":"2019-08-26T13:42:29.308Z","updated":"2019-08-26T05:57:26.713Z","comments":false,"path":"books/index.html","permalink":"http://www.ithelei.com/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-08-26T13:42:29.311Z","updated":"2019-08-26T05:57:26.715Z","comments":false,"path":"categories/index.html","permalink":"http://www.ithelei.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-08-26T14:06:33.045Z","updated":"2019-08-26T05:57:26.717Z","comments":true,"path":"links/index.html","permalink":"http://www.ithelei.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2019-08-26T13:42:29.318Z","updated":"2019-08-26T05:57:26.720Z","comments":false,"path":"repository/index.html","permalink":"http://www.ithelei.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-08-26T13:42:29.321Z","updated":"2019-08-26T05:57:26.722Z","comments":false,"path":"tags/index.html","permalink":"http://www.ithelei.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"某个应用的CPU使用率居然打满,我该怎么办？","slug":"CPU使用率居然打满","date":"2019-09-02T01:20:45.000Z","updated":"2019-09-08T08:45:44.319Z","comments":true,"path":"2019/09/02/CPU使用率居然打满/","link":"","permalink":"http://www.ithelei.com/2019/09/02/CPU使用率居然打满/","excerpt":"","text":"CPU 使用率是单位时间内 CPU 使用情况的统计，以百分比的方式展示。那么，作为最常用也是最熟悉的 CPU 指标，你能说出 CPU 使用率到底是怎么算出来的吗？再有，诸如 top、ps 之类的性能工具展示的 %user、%nice、 %system、%iowait 、%steal 等等 CPU 使用率在上一篇文章我曾提到，Linux 作为一个多任务操作系统，将每个 CPU 的时间划分为很短的时间片，再通过调度器轮流分配给各个任务使用，因此造成多任务同时运行的错觉。 为了维护 CPU 时间，Linux 通过事先定义的节拍率（内核中表示为 HZ），触发时间中断，并使用全局变量 Jiffies 记录了开机以来的节拍数。每发生一次时间中断，Jiffies 的值就加 1。 节拍率 HZ 是内核的可配选项，可以设置为 100、250、1000 等。不同的系统可能设置不同数值，你可以通过查询 /boot/config 内核选项来查看它的配置值。比如在我的系统中，节拍率设置成了 250，也就是每秒钟触发 250 次时间中断。 `grep &apos;CONFIG_HZ=&apos; /boot/config-$(uname -r) CONFIG_HZ=250 `同时，正因为节拍率 HZ 是内核选项，所以用户空间程序并不能直接访问。为了方便用户空间程序，内核还提供了一个用户空间节拍率 USERHZ，它总是固定为 100，也就是 1/100 秒。这样，用户空间程序并不需要关心内核中 HZ 被设置成了多少，因为它看到的总是固定值 USERHZ。 Linux 通过 /proc 虚拟文件系统，向用户空间提供了系统内部状态的信息，而 /proc/stat 提供的就是系统的 CPU 和任务统计信息。比方说，如果你只关注 CPU 的话，可以执行下面的命令： `# 只保留各个 CPU 的数据 $ cat /proc/stat | grep ^cpu cpu 280580 7407 286084 172900810 83602 0 583 0 0 0 cpu0 144745 4181 176701 86423902 52076 0 301 0 0 0 cpu1 135834 3226 109383 86476907 31525 0 282 0 0 0 `这里的输出结果是一个表格。其中，第一列表示的是 CPU 编号，如 cpu0、cpu1 ，而第一行没有编号的 cpu ，表示的是所有 CPU 的累加。其他列则表示不同场景下 CPU 的累加节拍数，它的单位是 USERHZ，也就是 10 ms（1/100 秒），所以这其实就是不同场景下的 CPU 时间。 当然，这里每一列都要记住，有需要的时候，查询 man proc 就可以。不过，你要清楚 man proc 文档里每一列的涵义，它们都是 CPU 使用率相关的重要指标，你还会在很多其他的性能工具中看到它们。下面，我来依次解读一下。 user（通常缩写为 us），代表用户态 CPU 时间。注意，它不包括下面的 nice 时间，但包括了 guest 时间。 nice（通常缩写为 ni），代表低优先级用户态 CPU 时间，也就是进程的 nice 值被调整为 1-19 之间时的 CPU 时间。这里注意，nice 可取值范围是 -20 到 19，数值越大，优先级反而越低。 system（通常缩写为 sys），代表内核态 CPU 时间。 idle（通常缩写为 id），代表空闲时间。注意，它不包括等待 I/O 的时间（iowait）。 iowait（通常缩写为 wa），代表等待 I/O 的 CPU 时间。 irq（通常缩写为 hi），代表处理硬中断的 CPU 时间。 softirq（通常缩写为 si），代表处理软中断的 CPU 时间。 steal（通常缩写为 st），代表当系统运行在虚拟机中的时候，被其他虚拟机占用的 CPU 时间。 guest（通常缩写为 guest），代表通过虚拟化运行其他操作系统的时间，也就是运行虚拟机的 CPU 时间。 guest_nice（通常缩写为 gnice），代表以低优先级运行虚拟机的时间。 而我们通常所说的CPU 使用率，就是除了空闲时间外的其他时间占总 CPU 时间的百分比，用公式来表示就是： 根据这个公式，我们就可以从 /proc/stat 中的数据，很容易地计算出 CPU 使用率。当然，也可以用每一个场景的 CPU 时间，除以总的 CPU 时间，计算出每个场景的 CPU 使用率。 不过先不要着急计算，你能说出，直接用 /proc/stat 的数据，算的是什么时间段的 CPU 使用率吗？ 看到这里，你应该想起来了，这是开机以来的节拍数累加值，所以直接算出来的，是开机以来的平均 CPU 使用率，一般没啥参考价值。 事实上，为了计算 CPU 使用率，性能工具一般都会取间隔一段时间（比如 3 秒）的两次值，作差后，再计算出这段时间内的平均 CPU 使用率，即 这个公式，就是我们用各种性能工具所看到的 CPU 使用率的实际计算方法。 现在，我们知道了系统 CPU 使用率的计算方法，那进程的呢？跟系统的指标类似，Linux 也给每个进程提供了运行情况的统计信息，也就是 /proc/[pid]/stat。不过，这个文件包含的数据就比较丰富了，总共有 52 列的数据。 当然，不用担心，因为你并不需要掌握每一列的含义。还是那句话，需要的时候，查 man proc 就行。 回过头来看，是不是说要查看 CPU 使用率，就必须先读取 /proc/stat 和 /proc/[pid]/stat 这两个文件，然后再按照上面的公式计算出来呢？ 当然不是，各种各样的性能分析工具已经帮我们计算好了。不过要注意的是，性能分析工具给出的都是间隔一段时间的平均 CPU 使用率，所以要注意间隔时间的设置，特别是用多个工具对比分析时，你一定要保证它们用的是相同的间隔时间。 比如，对比一下 top 和 ps 这两个工具报告的 CPU 使用率，默认的结果很可能不一样，因为 top 默认使用 3 秒时间间隔，而 ps 使用的却是进程的整个生命周期。 怎么查看 CPU 使用率知道了 CPU 使用率的含义后，我们再来看看要怎么查看 CPU 使用率。说到查看 CPU 使用率的工具，我猜你第一反应肯定是 top 和 ps。的确，top 和 ps 是最常用的性能分析工具： top 显示了系统总体的 CPU 和内存使用情况，以及各个进程的资源使用情况。 ps 则只显示了每个进程的资源使用情况。 比如，top 的输出格式为： `# 默认每 3 秒刷新一次 $ top top - 11:58:59 up 9 days, 22:47, 1 user, load average: 0.03, 0.02, 0.00 Tasks: 123 total, 1 running, 72 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.3 us, 0.3 sy, 0.0 ni, 99.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 8169348 total, 5606884 free, 334640 used, 2227824 buff/cache KiB Swap: 0 total, 0 free, 0 used. 7497908 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 78088 9288 6696 S 0.0 0.1 0:16.83 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.05 kthreadd 4 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/0:0H ... `这个输出结果中，第三行 %Cpu 就是系统的 CPU 使用率，具体每一列的含义上一节都讲过，只是把 CPU 时间变换成了 CPU 使用率，我就不再重复讲了。不过需要注意，top 默认显示的是所有 CPU 的平均值，这个时候你只需要按下数字 1 ，就可以切换到每个 CPU 的使用率了。 继续往下看，空白行之后是进程的实时信息，每个进程都有一个 %CPU 列，表示进程的 CPU 使用率。它是用户态和内核态 CPU 使用率的总和，包括进程用户空间使用的 CPU、通过系统调用执行的内核空间 CPU 、以及在就绪队列等待运行的 CPU。在虚拟化环境中，它还包括了运行虚拟机占用的 CPU。 所以，到这里我们可以发现， top 并没有细分进程的用户态 CPU 和内核态 CPU。那要怎么查看每个进程的详细情况呢？你应该还记得上一节用到的 pidstat 吧，它正是一个专门分析每个进程 CPU 使用情况的工具。 比如，下面的 pidstat 命令，就间隔 1 秒展示了进程的 5 组 CPU 使用率，包括： 用户态 CPU 使用率 （%usr）； 内核态 CPU 使用率（%system）； 运行虚拟机 CPU 使用率（%guest）； 等待 CPU 使用率（%wait）； 以及总的 CPU 使用率（%CPU） 最后的 Average 部分，还计算了 5 组数据的平均值。 `# 每隔 1 秒输出一组数据，共输出 5 组 $ pidstat 1 5 15:56:02 UID PID %usr %system %guest %wait %CPU CPU Command 15:56:03 0 15006 0.00 0.99 0.00 0.00 0.99 1 dockerd ... Average: UID PID %usr %system %guest %wait %CPU CPU Command Average: 0 15006 0.00 0.99 0.00 0.00 0.99 - dockerd `CPU 使用率过高怎么办？通过 top、ps、pidstat 等工具，你能够轻松找到 CPU 使用率较高（比如 100% ）的进程。接下来，你可能又想知道，占用 CPU 的到底是代码里的哪个函数呢？找到它，你才能更高效、更针对性地进行优化。 我猜你第一个想到的，应该是 GDB（The GNU Project Debugger）， 这个功能强大的程序调试利器。的确，GDB 在调试程序错误方面很强大。但是，我又要来“挑刺”了。请你记住，GDB 并不适合在性能分析的早期应用。 为什么呢？因为 GDB 调试程序的过程会中断程序运行，这在线上环境往往是不允许的。所以，GDB 只适合用在性能分析的后期，当你找到了出问题的大致函数后，线下再借助它来进一步调试函数内部的问题。 那么哪种工具适合在第一时间分析进程的 CPU 问题呢？我的推荐是 perf。perf 是 Linux 2.6.31 以后内置的性能分析工具。它以性能事件采样为基础，不仅可以分析系统的各种事件和内核性能，还可以用来分析指定应用程序的性能问题。 使用 perf 分析 CPU 性能问题，我来说两种最常见、也是我最喜欢的用法。 第一种常见用法是 perf top，类似于 top，它能够实时显示占用 CPU 时钟最多的函数或者指令，因此可以用来查找热点函数，使用界面如下所示： `perf top Samples: 833 of event &apos;cpu-clock&apos;, Event count (approx.): 97742399 Overhead Shared Object Symbol 7.28% perf [.] 0x00000000001f78a4 4.72% [kernel] [k] vsnprintf 4.32% [kernel] [k] module_get_kallsym 3.65% [kernel] [k] _raw_spin_unlock_irqrestore ... `输出结果中，第一行包含三个数据，分别是采样数（Samples）、事件类型（event）和事件总数量（Event count）。比如这个例子中，perf 总共采集了 833 个 CPU 时钟事件，而总事件数则为 97742399。 另外，采样数需要我们特别注意。如果采样数过少（比如只有十几个），那下面的排序和百分比就没什么实际参考价值了。 再往下看是一个表格式样的数据，每一行包含四列，分别是： 第一列 Overhead ，是该符号的性能事件在所有采样中的比例，用百分比来表示。 第二列 Shared ，是该函数或指令所在的动态共享对象（Dynamic Shared Object），如内核、进程名、动态链接库名、内核模块名等。 第三列 Object ，是动态共享对象的类型。比如 [.] 表示用户空间的可执行程序、或者动态链接库，而 [k] 则表示内核空间。 最后一列 Symbol 是符号名，也就是函数名。当函数名未知时，用十六进制的地址来表示。 还是以上面的输出为例，我们可以看到，占用 CPU 时钟最多的是 perf 工具自身，不过它的比例也只有 7.28%，说明系统并没有 CPU 性能问题。 perf top 的使用你应该很清楚了吧。 接着再来看第二种常见用法，也就是 perf record 和 perf report。 perf top 虽然实时展示了系统的性能信息，但它的缺点是并不保存数据，也就无法用于离线或者后续的分析。而 perf record 则提供了保存数据的功能，保存后的数据，需要你用 perf report 解析展示。 `perf record # 按 Ctrl+C 终止采样 [ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.452 MB perf.data (6093 samples) ] $ perf report # 展示类似于 perf top 的报告 `在实际使用中，我们还经常为 perf top 和 perf record 加上 -g 参数，开启调用关系的采样，方便我们根据调用链来分析性能问题。 案例下面我们就以 Nginx + PHP 的 Web 服务为例，来看看当你发现 CPU 使用率过高的问题后，要怎么使用 top 等工具找出异常的进程，又要怎么利用 perf 找出引发性能问题的函数。 准备以下案例基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。我使用的案例环境如下所示： 机器配置：2 CPU，8GB 内存 预先安装 docker、sysstat、perf、ab 等工具，如 apt install docker.io sysstat linux-tools-common apache2-utils 我先简单介绍一下这次新使用的工具 ab。ab（apache bench）是一个常用的 HTTP 服务性能测试工具，这里用来模拟 Ngnix 的客户端。由于 Nginx 和 PHP 的配置比较麻烦，我把它们打包成了两个 Docker 镜像https://github.com/feiskyer/linux-perf-examples/tree/master/nginx-high-cpu，这样只需要运行两个容器，就可以得到模拟环境。 注意，这个案例要用到两台虚拟机，如下图所示： 其中一台用作 Web 服务器，来模拟性能问题；另一台用作 Web 服务器的客户端，来给 Web 服务增加压力请求。使用两台虚拟机是为了相互隔离，避免“交叉感染”。 接下来，我们打开两个终端，分别 SSH 登录到两台机器上，并安装上面提到的工具。 还是同样的“配方”。下面的所有命令，都默认假设以 root 用户运行，如果你是普通用户身份登陆系统，一定要先运行 sudo su root 命令切换到 root 用户。到这里，准备工作就完成了。 不过，操作之前，我还想再说一点。这次案例中 PHP 应用的核心逻辑比较简单，大部分人一眼就可以看出问题，但你要知道，实际生产环境中的源码就复杂多了。 所以，我希望你在按照步骤操作之前，先不要查看源码（避免先入为主），而是把它当成一个黑盒来分析。这样，你可以更好地理解整个解决思路，怎么从系统的资源使用问题出发，分析出瓶颈所在的应用、以及瓶颈在应用中的大概位置。 操作和分析接下来，我们正式进入操作环节。 首先，在第一个终端执行下面的命令来运行 Nginx 和 PHP 应用： `docker run --name nginx -p 10000:80 -itd feisky/nginx $ docker run --name phpfpm -itd --network container:nginx feisky/php-fpm `然后，在第二个终端使用 curl 访问 http://[VM1 的 IP]:10000，确认 Nginx 已正常启动。你应该可以看到 It works! 的响应。 `# 192.168.0.10 是第一台虚拟机的 IP 地址 $ curl http://192.168.0.10:10000/ It works! `接着，我们来测试一下这个 Nginx 服务的性能。在第二个终端运行下面的 ab 命令： `# 并发 10 个请求测试 Nginx 性能，总共测试 100 个请求 $ ab -c 10 -n 100 http://192.168.0.10:10000/ This is ApacheBench, Version 2.3 &lt;$Revision: 1706008 $&gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, ... Requests per second: 11.63 [#/sec] (mean) Time per request: 859.942 [ms] (mean) ... `从 ab 的输出结果我们可以看到，Nginx 能承受的每秒平均请求数只有 11.63。，这也太差了吧。那到底是哪里出了问题呢？我们用 top 和 pidstat 再来观察下。 这次，我们在第二个终端，将测试的请求总数增加到 10000。这样当你在第一个终端使用性能分析工具时， Nginx 的压力还是继续。 继续在第二个终端，运行 ab 命令： `ab -c 10 -n 10000 http://10.240.0.5:10000/ `接着，回到第一个终端运行 top 命令，并按下数字 1 ，切换到每个 CPU 的使用率： `top ... %Cpu0 : 98.7 us, 1.3 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu1 : 99.3 us, 0.7 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st ... PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 21514 daemon 20 0 336696 16384 8712 R 41.9 0.2 0:06.00 php-fpm 21513 daemon 20 0 336696 13244 5572 R 40.2 0.2 0:06.08 php-fpm 21515 daemon 20 0 336696 16384 8712 R 40.2 0.2 0:05.67 php-fpm 21512 daemon 20 0 336696 13244 5572 R 39.9 0.2 0:05.87 php-fpm 21516 daemon 20 0 336696 16384 8712 R 35.9 0.2 0:05.61 php-fpm `这里可以看到，系统中有几个 php-fpm 进程的 CPU 使用率加起来接近 200%；而每个 CPU 的用户使用率（us）也已经超过了 98%，接近饱和。这样，我们就可以确认，正是用户空间的 php-fpm 进程，导致 CPU 使用率骤升。 那再往下走，怎么知道是 php-fpm 的哪个函数导致了 CPU 使用率升高呢？我们来用 perf 分析一下。在第一个终端运行下面的 perf 命令： `# -g 开启调用关系分析，-p 指定 php-fpm 的进程号 21515 $ perf top -g -p 21515 `按方向键切换到 php-fpm，再按下回车键展开 php-fpm 的调用关系，你会发现，调用关系最终到了 sqrt 和 addfunction。看来，我们需要从这两个函数入手了。 我们拷贝出 Nginx 应用的源码，看看是不是调用了这两个函数： `# 从容器 phpfpm 中将 PHP 源码拷贝出来 $ docker cp phpfpm:/app . # 使用 grep 查找函数调用 $ grep sqrt -r app/ # 找到了 sqrt 调用 app/index.php: $x += sqrt($x); $ grep add_function -r app/ # 没找到 add_function 调用，这其实是 PHP 内置函数 `OK，原来只有 sqrt 函数在 app/index.php 文件中调用了。那最后一步，我们就该看看这个文件的源码了： `cat app/index.php &lt;?php // test only. $x = 0.0001; for ($i = 0; $i &lt;= 1000000; $i++) { $x += sqrt($x); } echo &quot;It works!&quot; `呀，有没有发现问题在哪里呢？我想你要笑话我了，居然犯了一个这么傻的错误，测试代码没删就直接发布应用了。为了方便你验证优化后的效果，我把修复后的应用也打包成了一个 Docker 镜像，你可以在第一个终端中执行下面的命令来运行它： `# 停止原来的应用 $ docker rm -f nginx phpfpm # 运行优化后的应用 $ docker run --name nginx -p 10000:80 -itd feisky/nginx:cpu-fix $ docker run --name phpfpm -itd --network container:nginx feisky/php-fpm:cpu-fix `接着，到第二个终端来验证一下修复后的效果。首先 Ctrl+C 停止之前的 ab 命令后，再运行下面的命令： `ab -c 10 -n 10000 http://10.240.0.5:10000/ ... Complete requests: 10000 Failed requests: 0 Total transferred: 1720000 bytes HTML transferred: 90000 bytes Requests per second: 2237.04 [#/sec] (mean) Time per request: 4.470 [ms] (mean) Time per request: 0.447 [ms] (mean, across all concurrent requests) Transfer rate: 375.75 [Kbytes/sec] received ... `从这里你可以发现，现在每秒的平均请求数，已经从原来的 11 变成了 2237。 你看，就是这么很傻的一个小问题，却会极大的影响性能，并且查找起来也并不容易吧。当然，找到问题后，解决方法就简单多了，删除测试代码就可以了。 小结CPU 使用率是最直观和最常用的系统性能指标，更是我们在排查性能问题时，通常会关注的第一个指标。所以我们更要熟悉它的含义，尤其要弄清楚用户（%user）、Nice（%nice）、系统（%system） 、等待 I/O（%iowait） 、中断（%irq）以及软中断（%softirq）这几种不同 CPU 的使用率。比如说： 用户 CPU 和 Nice CPU 高，说明用户态进程占用了较多的 CPU，所以应该着重排查进程的性能问题。 系统 CPU 高，说明内核态占用了较多的 CPU，所以应该着重排查内核线程或者系统调用的性能问题。 I/O 等待 CPU 高，说明等待 I/O 的时间比较长，所以应该着重排查系统存储是不是出现了 I/O 问题。 软中断和硬中断高，说明软中断或硬中断的处理程序占用了较多的 CPU，所以应该着重排查内核中的中断服务程序。 碰到 CPU 使用率升高的问题，你可以借助 top、pidstat 等工具，确认引发 CPU 性能问题的来源；再使用 perf 等工具，排查出引起性能问题的具体函数。","categories":[{"name":"CPU","slug":"CPU","permalink":"http://www.ithelei.com/categories/CPU/"}],"tags":[{"name":"CPU","slug":"CPU","permalink":"http://www.ithelei.com/tags/CPU/"}]},{"title":"传统xml配置方式","slug":"传统xml配置方式","date":"2019-09-02T01:20:45.000Z","updated":"2019-09-03T15:00:41.043Z","comments":true,"path":"2019/09/02/传统xml配置方式/","link":"","permalink":"http://www.ithelei.com/2019/09/02/传统xml配置方式/","excerpt":"","text":"idea传统xml配置方式新建项目 pom.xml `&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.javaboy&lt;/groupId&gt; &lt;artifactId&gt;xmlssm&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.2.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; `依赖: applicationContext.xml `&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;org.javaboy&quot; use-default-filters=&quot;true&quot;&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt; &lt;/beans&gt;`spring-servlet.xml `&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;!--springmvc容器 是spring的子容器 springmvc可以访问spring容器的东西。--&gt; &lt;context:component-scan base-package=&quot;org.javaboy&quot; use-default-filters=&quot;false&quot;&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt; &lt;mvc:annotation-driven/&gt; &lt;/beans&gt;`web.xml `&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot; version=&quot;3.1&quot;&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;/web-app&gt; `HelloController `package org.javaboy.controller; import org.javaboy.service.HelloService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.Mapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RestController; /** * */ @RestController public class HelloController { @Autowired HelloService helloService ; @RequestMapping(method = RequestMethod.GET) public String hello(){ return helloService.sayhello(); } } `HelloService `package org.javaboy.service; import org.springframework.stereotype.Service; @Service public class HelloService { public String sayhello(){ return &quot;ithelei&quot;; } } `","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.ithelei.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.ithelei.com/tags/SpringBoot/"}]},{"title":"经常说的-CPU-上下文切换是什么意思？（下）","slug":"经常说的-CPU-上下文切换是什么意思？（下）","date":"2019-09-01T01:20:45.000Z","updated":"2019-09-01T11:48:34.973Z","comments":true,"path":"2019/09/01/经常说的-CPU-上下文切换是什么意思？（下）/","link":"","permalink":"http://www.ithelei.com/2019/09/01/经常说的-CPU-上下文切换是什么意思？（下）/","excerpt":"","text":"CPU 上下文切换的工作原理。简单回顾一下，CPU 上下文切换是保证 Linux 系统正常工作的一个核心功能，按照不同场景，可以分为进程上下文切换、线程上下文切换和中断上下文切换。具体的概念和区别，你也要在脑海中过一遍，忘了的话及时查看上一篇。 怎么查看系统的上下文切换情况过多的上下文切换，会把 CPU 时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，缩短进程真正运行的时间，成了系统性能大幅下降的一个元凶。 既然上下文切换对系统性能影响那么大，你肯定迫不及待想知道，到底要怎么查看上下文切换呢？在这里，我们可以使用 vmstat 这个工具，来查询系统的上下文切换情况。 vmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。 比如，下面就是一个 vmstat 的使用示例： # 每隔 5 秒输出 1 组数据 我们一起来看这个结果，你可以先试着自己解读每列的含义。在这里，我重点强调下，需要特别关注的四列内容： cs（context switch）是每秒上下文切换的次数。 in（interrupt）则是每秒中断的次数。 r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。 b（Blocked）则是处于不可中断睡眠状态的进程数。 可以看到，这个例子中的上下文切换次数 cs 是 2 次，而系统中断次数 in 则是 3 次，而就绪队列长度 r 和不可中断状态进程数 b 都是 0。 vmstat 只给出了系统总体的上下文切换情况，要想查看每个进程的详细情况，就需要使用我们前面提到过的 pidstat 了。给它加上 -w 选项，你就可以查看每个进程上下文切换的情况了。 比如说：pidstat -w 5 这个结果中有两列内容是我们的重点关注对象。一个是 cswch ，表示每秒自愿上下文切换（voluntary context switches）的次数，另一个则是 nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数。 这两个概念你一定要牢牢记住，因为它们意味着不同的性能问题： 所谓自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。 而非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。 案例分析知道了怎么查看这些指标，另一个问题又来了，上下文切换频率是多少次才算正常呢？别急着要答案，同样的，我们先来看一个上下文切换的案例。通过案例实战演练，你自己就可以分析并找出这个标准了。 准备我们将使用 sysbench 来模拟系统多线程调度切换的情况。 sysbench 是一个多线程的基准测试工具，一般用来评估不同系统参数下的数据库负载情况。当然，在这次案例中，我们只把它当成一个异常进程来看，作用是模拟上下文切换过多的问题。 下面的案例基于 Ubuntu 18.04，当然，其他的 Linux 系统同样适用。我使用的案例环境如下所示： 机器配置：2 CPU，8GB 内存 预先安装 sysbench 和 sysstat 包，如 apt install sysbench sysstat 正式操作开始前，你需要打开三个终端，登录到同一台 Linux 机器中，并安装好上面提到的两个软件包。包的安装，可以先 Google 一下自行解决，如果仍然有问题的，在留言区写下你的情况。 另外注意，下面所有命令，都默认以 root 用户运行。所以，如果你是用普通用户登陆的系统，记住先运行 sudo su root 命令切换到 root 用户。 安装完成后，你可以先用 vmstat 看一下空闲系统的上下文切换次数：vmstat 1 1 这里你可以看到，现在的上下文切换次数 cs 是 0，而中断次数 in 是 0，r 和 b 都是 0。因为这会儿我并没有运行其他任务，所以它们就是空闲系统的上下文切换次数。 操作和分析接下来，我们正式进入实战操作。 首先，在第一个终端里运行 sysbench ，模拟系统多线程调度的瓶颈： # 以 10 个线程运行 5 分钟的基准测试，模拟多线程切换的问题$ sysbench –threads=10 –max-time=300 threads run 接着，在第二个终端运行 vmstat ，观察上下文切换情况： vmstat 1 你应该可以发现，cs 列的上下文切换次数从之前的 35 骤然上升到了 139 万。同时，注意观察其他几个指标： r 列：就绪队列的长度已经到了 8，远远超过了系统 CPU 的个数 2，所以肯定会有大量的 CPU 竞争。 us（user）和 sy（system）列：这两列的 CPU 使用率加起来上升到了 100%，其中系统 CPU 使用率，也就是 sy 列高达 84%，说明 CPU 主要是被内核占用了。 in 列：中断次数也上升到了 1 万左右，说明中断处理也是个潜在的问题。 综合这几个指标，我们可以知道，系统的就绪队列过长，也就是正在运行和等待 CPU 的进程数过多，导致了大量的上下文切换，而上下文切换又导致了系统 CPU 的占用率升高。 那么到底是什么进程导致了这些问题呢？ 我们继续分析，在第三个终端再用 pidstat 来看一下， CPU 和进程上下文切换的情况： `# 每隔 1 秒输出 1 组数据（需要 Ctrl+C 才结束） # -w 参数表示输出进程切换指标，而 -u 参数则表示输出 CPU 使用指标 $ pidstat -w -u 1 08:06:33 UID PID %usr %system %guest %wait %CPU CPU Command 08:06:34 0 10488 30.00 100.00 0.00 0.00 100.00 0 sysbench 08:06:34 0 26326 0.00 1.00 0.00 0.00 1.00 0 kworker/u4:2 08:06:33 UID PID cswch/s nvcswch/s Command 08:06:34 0 8 11.00 0.00 rcu_sched 08:06:34 0 16 1.00 0.00 ksoftirqd/1 08:06:34 0 471 1.00 0.00 hv_balloon 08:06:34 0 1230 1.00 0.00 iscsid 08:06:34 0 4089 1.00 0.00 kworker/1:5 08:06:34 0 4333 1.00 0.00 kworker/0:3 08:06:34 0 10499 1.00 224.00 pidstat 08:06:34 0 26326 236.00 0.00 kworker/u4:2 08:06:34 1000 26784 223.00 0.00 sshd `从 pidstat 的输出你可以发现，CPU 使用率的升高果然是 sysbench 导致的，它的 CPU 使用率已经达到了 100%。但上下文切换则是来自其他进程，包括非自愿上下文切换频率最高的 pidstat ，以及自愿上下文切换频率最高的内核线程 kworker 和 sshd。 不过，细心的你肯定也发现了一个怪异的事儿：pidstat 输出的上下文切换次数，加起来也就几百，比 vmstat 的 139 万明显小了太多。这是怎么回事呢？难道是工具本身出了错吗？ 别着急，在怀疑工具之前，我们再来回想一下，前面讲到的几种上下文切换场景。其中有一点提到， Linux 调度的基本单位实际上是线程，而我们的场景 sysbench 模拟的也是线程的调度问题，那么，是不是 pidstat 忽略了线程的数据呢？ 通过运行 man pidstat ，你会发现，pidstat 默认显示进程的指标数据，加上 -t 参数后，才会输出线程的指标。 所以，我们可以在第三个终端里， Ctrl+C 停止刚才的 pidstat 命令，再加上 -t 参数，重试一下看看： `# 每隔 1 秒输出一组数据（需要 Ctrl+C 才结束） # -wt 参数表示输出线程的上下文切换指标 $ pidstat -wt 1 08:14:05 UID TGID TID cswch/s nvcswch/s Command ... 08:14:05 0 10551 - 6.00 0.00 sysbench 08:14:05 0 - 10551 6.00 0.00 |__sysbench 08:14:05 0 - 10552 18911.00 103740.00 |__sysbench 08:14:05 0 - 10553 18915.00 100955.00 |__sysbench 08:14:05 0 - 10554 18827.00 103954.00 |__sysbench ... `现在你就能看到了，虽然 sysbench 进程（也就是主线程）的上下文切换次数看起来并不多，但它的子线程的上下文切换次数却有很多。看来，上下文切换罪魁祸首，还是过多的 sysbench 线程。 我们已经找到了上下文切换次数增多的根源，那是不是到这儿就可以结束了呢？ 当然不是。不知道你还记不记得，前面在观察系统指标时，除了上下文切换频率骤然升高，还有一个指标也有很大的变化。是的，正是中断次数。中断次数也上升到了 1 万，但到底是什么类型的中断上升了，现在还不清楚。我们接下来继续抽丝剥茧找源头。 既然是中断，我们都知道，它只发生在内核态，而 pidstat 只是一个进程的性能分析工具，并不提供任何关于中断的详细信息，怎样才能知道中断发生的类型呢？ 没错，那就是从 /proc/interrupts 这个只读文件中读取。/proc 实际上是 Linux 的一个虚拟文件系统，用于内核空间与用户空间之间的通信。/proc/interrupts 就是这种通信机制的一部分，提供了一个只读的中断使用情况。 我们还是在第三个终端里， Ctrl+C 停止刚才的 pidstat 命令，然后运行下面的命令，观察中断的变化情况： `# -d 参数表示高亮显示变化的区域 $ watch -d cat /proc/interrupts CPU0 CPU1 ... RES: 2450431 5279697 Rescheduling interrupts ... `观察一段时间，你可以发现，变化速度最快的是重调度中断（RES），这个中断类型表示，唤醒空闲状态的 CPU 来调度新的任务运行。这是多处理器系统（SMP）中，调度器用来分散任务到不同 CPU 的机制，通常也被称为处理器间中断（Inter-Processor Interrupts，IPI）。 所以，这里的中断升高还是因为过多任务的调度问题，跟前面上下文切换次数的分析结果是一致的。 通过这个案例，你应该也发现了多工具、多方面指标对比观测的好处。如果最开始时，我们只用了 pidstat 观测，这些很严重的上下文切换线程，压根儿就发现不了了。 现在再回到最初的问题，每秒上下文切换多少次才算正常呢？ 这个数值其实取决于系统本身的 CPU 性能。在我看来，如果系统的上下文切换次数比较稳定，那么从数百到一万以内，都应该算是正常的。但当上下文切换次数超过一万次，或者切换次数出现数量级的增长时，就很可能已经出现了性能问题。 这时，你还需要根据上下文切换的类型，再做具体分析。比方说： 自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 I/O 等其他问题； 非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢 CPU，说明 CPU 的确成了瓶颈； 中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看 /proc/interrupts 文件来分析具体的中断类型。 小结今天，我通过一个 sysbench 的案例，给你讲了上下文切换问题的分析思路。碰到上下文切换次数过多的问题时，我们可以借助 vmstat 、 pidstat 和 /proc/interrupts 等工具，来辅助排查性能问题的根源。","categories":[{"name":"CPU","slug":"CPU","permalink":"http://www.ithelei.com/categories/CPU/"}],"tags":[{"name":"CPU","slug":"CPU","permalink":"http://www.ithelei.com/tags/CPU/"}]},{"title":"经常说的-CPU-上下文切换是什么意思？（上）","slug":"经常说的-CPU-上下文切换是什么意思？（上）","date":"2019-09-01T01:20:45.000Z","updated":"2019-09-01T11:47:28.043Z","comments":true,"path":"2019/09/01/经常说的-CPU-上下文切换是什么意思？（上）/","link":"","permalink":"http://www.ithelei.com/2019/09/01/经常说的-CPU-上下文切换是什么意思？（上）/","excerpt":"","text":"理解平均负载（ Load Average），并用三个案例展示了不同场景下平均负载升高的分析方法。这其中，多个进程竞争 CPU 就是一个经常被我们忽视的问题。我想你一定很好奇，进程在竞争 CPU 的时候并没有真正运行，为什么还会导致系统的负载升高呢？看到今天的主题，你应该已经猜到了，CPU 上下文切换就是罪魁祸首。 我们都知道，Linux 是一个多任务操作系统，它支持远大于 CPU 数量的任务同时运行。当然，这些任务实际上并不是真的在同时运行，而是因为系统在很短的时间内，将 CPU 轮流分配给它们，造成多任务同时运行的错觉。 而在每个任务运行前，CPU 都需要知道任务从哪里加载、又从哪里开始运行，也就是说，需要系统事先帮它设置好CPU 寄存器和程序计数器（Program Counter，PC）。 CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做CPU 上下文。 知道了什么是 CPU 上下文，我想你也很容易理解 CPU 上下文切换。CPU 上下文切换，就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。 而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。 我猜肯定会有人说，CPU 上下文切换无非就是更新了 CPU 寄存器的值嘛，但这些寄存器，本身就是为了快速运行任务而设计的，为什么会影响系统的 CPU 性能呢？ 在回答这个问题前，不知道你有没有想过，操作系统管理的这些“任务”到底是什么呢？CPU 寄存器也许你会说，任务就是进程，或者说任务就是线程。是的，进程和线程正是最常见的任务。但是除此之外，还有没有其他的任务呢？ 不要忘了，硬件通过触发信号，会导致中断处理程序的调用，也是一种常见的任务。 所以，根据任务的不同，CPU 的上下文切换就可以分为几个不同的场景，也就是进程上下文切换、线程上下文切换以及中断上下文切换。 进程上下文切换Linux 按照特权等级，把进程的运行空间分为内核空间和用户空间，分别对应着下图中， CPU 特权等级的 Ring 0 和 Ring 3。 内核空间（Ring 0）具有最高权限，可以直接访问所有资源； 用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用陷入到内核中，才能访问这些特权资源。 换个角度看，也就是说，进程既可以在用户空间运行，又可以在内核空间中运行。进程在用户空间运行时，被称为进程的用户态，而陷入内核空间的时候，被称为进程的内核态。 从用户态到内核态的转变，需要通过系统调用来完成。比如，当我们查看文件内容时，就需要多次系统调用来完成：首先调用 open() 打开文件，然后调用 read() 读取文件内容，并调用 write() 将内容写到标准输出，最后再调用 close() 关闭文件。 那么，系统调用的过程有没有发生 CPU 上下文的切换呢？答案自然是肯定的。 CPU 寄存器里原来用户态的指令位置，需要先保存起来。接着，为了执行内核态代码，CPU 寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。 而系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。所以，一次系统调用的过程，其实是发生了两次 CPU 上下文切换。 不过，需要注意的是，系统调用过程中，并不会涉及到虚拟内存等进程用户态的资源，也不会切换进程。这跟我们通常所说的进程上下文切换是不一样的： 进程上下文切换，是指从一个进程切换到另一个进程运行。 而系统调用过程中一直是同一个进程在运行。 所以，系统调用过程通常称为特权模式切换，而不是上下文切换。但实际上，系统调用过程中，CPU 的上下文切换还是无法避免的。 那么，进程上下文切换跟系统调用又有什么区别呢？ 首先，你需要知道，进程是由内核来管理和调度的，进程的切换只能发生在内核态。所以，进程的上下文不仅包括了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的状态。 因此，进程的上下文切换就比系统调用时多了一步：在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该进程的虚拟内存、栈等保存下来；而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈。 如下图所示，保存上下文和恢复上下文的过程并不是“免费”的，需要内核在 CPU 上运行才能完成。 根据 Tsuna 的测试报告，每次上下文切换都需要几十纳秒到数微秒的 CPU 时间。这个时间还是相当可观的，特别是在进程上下文切换次数较多的情况下，很容易导致 CPU 将大量时间耗费在寄存器、内核栈以及虚拟内存等资源的保存和恢复上，进而大大缩短了真正运行进程的时间。这也正是上一节中我们所讲的，导致平均负载升高的一个重要因素。 另外，我们知道， Linux 通过 TLB（Translation Lookaside Buffer）来管理虚拟内存到物理内存的映射关系。当虚拟内存更新后，TLB 也需要刷新，内存的访问也会随之变慢。特别是在多处理器系统上，缓存是被多个处理器共享的，刷新缓存不仅会影响当前处理器的进程，还会影响共享缓存的其他处理器的进程。 知道了进程上下文切换潜在的性能问题后，我们再来看，究竟什么时候会切换进程上下文。 显然，进程切换时才需要切换上下文，换句话说，只有在进程调度的时候，才需要切换上下文。Linux 为每个 CPU 都维护了一个就绪队列，将活跃进程（即正在运行和正在等待 CPU 的进程）按照优先级和等待 CPU 的时间排序，然后选择最需要 CPU 的进程，也就是优先级最高和等待 CPU 时间最长的进程来运行。 那么，进程在什么时候才会被调度到 CPU 上运行呢？ 最容易想到的一个时机，就是进程执行完终止了，它之前使用的 CPU 会释放出来，这个时候再从就绪队列里，拿一个新的进程过来运行。其实还有很多其他场景，也会触发进程调度，在这里我给你逐个梳理下。 其一，为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。 其二，进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。 其三，当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。 其四，当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。 最后一个，发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。 了解这几个场景是非常有必要的，因为一旦出现上下文切换的性能问题，它们就是幕后凶手。 线程上下文切换说完了进程的上下文切换，我们再来看看线程相关的问题。 线程与进程最大的区别在于，线程是调度的基本单位，而进程则是资源拥有的基本单位。说白了，所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源。所以，对于线程和进程，我们可以这么理解： 当进程只有一个线程时，可以认为进程就等于线程。 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。 另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。 这么一来，线程的上下文切换其实就可以分为两种情况： 第一种， 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。 第二种，前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。 到这里你应该也发现了，虽然同为上下文切换，但同进程内的线程切换，要比多进程间的切换消耗更少的资源，而这，也正是多线程代替多进程的一个优势。 中断上下文切换除了前面两种上下文切换，还有一个场景也会切换 CPU 上下文，那就是中断。 为了快速响应硬件的事件，中断处理会打断进程的正常调度和执行，转而调用中断处理程序，响应设备事件。而在打断其他进程时，就需要将进程当前的状态保存下来，这样在中断结束后，进程仍然可以从原来的状态恢复运行。 跟进程上下文不同，中断上下文切换并不涉及到进程的用户态。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。中断上下文，其实只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等。 对同一个 CPU 来说，中断处理比进程拥有更高的优先级，所以中断上下文切换并不会与进程上下文切换同时发生。同样道理，由于中断会打断正常进程的调度和执行，所以大部分中断处理程序都短小精悍，以便尽可能快的执行结束。 另外，跟进程上下文切换一样，中断上下文切换也需要消耗 CPU，切换次数过多也会耗费大量的 CPU，甚至严重降低系统的整体性能。所以，当你发现中断次数过多时，就需要注意去排查它是否会给你的系统带来严重的性能问题。 小结总结一下，不管是哪种场景导致的上下文切换，你都应该知道： CPU 上下文切换，是保证 Linux 系统正常工作的核心功能之一，一般情况下不需要我们特别关注。 但过多的上下文切换，会把 CPU 时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，从而缩短进程真正运行的时间，导致系统的整体性能大幅下降。","categories":[{"name":"CPU","slug":"CPU","permalink":"http://www.ithelei.com/categories/CPU/"}],"tags":[{"name":"CPU","slug":"CPU","permalink":"http://www.ithelei.com/tags/CPU/"}]},{"title":"能力图谱","slug":"能力图谱","date":"2019-09-01T01:20:45.000Z","updated":"2019-09-01T01:24:41.711Z","comments":true,"path":"2019/09/01/能力图谱/","link":"","permalink":"http://www.ithelei.com/2019/09/01/能力图谱/","excerpt":"","text":"工程师能力图谱： 技术经理能力图谱 技术总监能力图谱 CTO能力图谱","categories":[{"name":"能力图谱","slug":"能力图谱","permalink":"http://www.ithelei.com/categories/能力图谱/"}],"tags":[{"name":"能力图谱","slug":"能力图谱","permalink":"http://www.ithelei.com/tags/能力图谱/"}]},{"title":"ECS上搭建Docker（CentOS7）","slug":"ECS上搭建Docker（CentOS7）","date":"2019-08-31T13:27:24.000Z","updated":"2019-08-31T15:02:39.023Z","comments":true,"path":"2019/08/31/ECS上搭建Docker（CentOS7）/","link":"","permalink":"http://www.ithelei.com/2019/08/31/ECS上搭建Docker（CentOS7）/","excerpt":"","text":"本文介绍在CentOS系统上部署Docker的过程。 背景信息本教程适用于熟悉Linux操作系统。 本教程示例步骤中使用的操作系统版本为CentOS 7.2 64 3.10.0-514.6.2.el7.x86_64 说明 Docker要求64位的系统且内核版本至少为3.10。部署Docker完成以下操作，部署Docker： 添加yum源。 yum install epel-release –y yum clean all yum list 安装并运行Docker。 yum install docker-io –y systemctl start docker 检查安装结果。 docker info 出现以下说明信息则表明安装成功。 使用DockerDocker有以下基本用法： 管理Docker守护进程。 systemctl start docker 运行Docker守护进程 systemctl stop docker 停止Docker守护进程 systemctl restart docker 重启Docker守护进程 管理镜像。本文使用的是来自阿里云仓库的Apache镜像。 docker pull registry.cn-hangzhou.aliyuncs.com/lxepoo/apache-php5 修改标签。由于阿里云仓库镜像的镜像名称很长，可以修改镜像标签以便记忆区分。 docker tag registry.cn-hangzhou.aliyuncs.com/lxepoo/apache-php5:latest aliweb:v1 查看已有镜像。 docker images 强制删除镜像。 docker rmi –f registry.cn-hangzhou.aliyuncs.com/lxepoo/apache-php5 管理容器。 进入容器。e1xxxxxxxxxe是执行docker images命令查询到的ImageId，使用docker run命令进入容器。 docker run –ti e1xxxxxxxxxe /bin/bash 退出容器。使用exit命令退出当前容器。 run命令加上–d参数可以在后台运行容器，–name指定容器命名为apache。 docker run -d –name apache e1xxxxxxxxxe 进入后台运行的容器。 docker exec -ti apache /bin/bash 将容器做成镜像。 docker commit containerID/containerName newImageName:tag 为了方便测试和恢复，将源镜像运行起来后，再做一个命名简单的镜像做测试。 docker commit 4c8066cd8c01 apachephp:v1 运行容器并将宿主机的8080端口映射到容器里去。 docker run -d -p 8080:80 apachephp:v1 在浏览器输入宿主机IP加8080端口访问测试，出现以下内容则说明运行成功。 制作镜像完成以下操作，制作镜像： 准备Dockerfile内容。 ` # vim Dockerfile FROM apachephp:v1 #声明基础镜像来源 MAINTAINER DTSTACK #声明镜像拥有者 RUN mkdir /dtstact #RUN后面接容器运行前需要执行的命令，由于Dockerfile文件不能超过127行，因此当命令较多时建议写到脚本中执行 ENTRYPOINT ping www.aliyun.com #开机启动命令，此处最后一个命令需要是可在前台持续执行的命令，否则容器后台运行时会因为命令执行完而退出。 ` 构建镜像。 ` docker build -t webcentos:v1 . # . 是Dockerfile文件的路径，不能忽略 docker images #查看是否创建成功 docker run –d webcentos:v1 #后台运行容器 docker ps #查看当前运行中的容器 docker ps –a #查看所有容器，包括未运行中的 docker logs CONTAINER ID/IMAGE #如未查看到刚才运行的容器，则用容器id或者名字查看启动日志排错 docker commit fb2844b6c070 dtstackweb:v1 #commit 后接容器id 和构建新镜像的名称和版本号。 docker images #列出本地（已下载的和本地创建的）镜像 docker push #将镜像推送至远程仓库，默认为 Docker Hub ` 将镜像推送到registry。 其中ImageId和镜像版本号请您根据自己的镜像信息进行填写。 ` docker login --username=dtstack_plus registry.cn-shanghai.aliyuncs.com #执行后输入镜像仓库密码 docker tag [ImageId] registry.cn-shanghai.aliyuncs.com/dtstack123/test:[镜像版本号] docker push registry.cn-shanghai.aliyuncs.com/dtstack123/test:[镜像版本号] `在镜像仓库能查看到镜像版本信息则说明推送成功。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://www.ithelei.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.ithelei.com/tags/Docker/"}]},{"title":"Tomcat服务启动非常缓慢","slug":"Tomcat服务启动非常缓慢","date":"2019-08-31T11:36:42.000Z","updated":"2019-08-31T12:16:45.879Z","comments":true,"path":"2019/08/31/Tomcat服务启动非常缓慢/","link":"","permalink":"http://www.ithelei.com/2019/08/31/Tomcat服务启动非常缓慢/","excerpt":"","text":"概述：本文主要介绍Tomcat服务启动非常缓慢的解决方法。 问题症状 Tomcat启动非常缓慢，查看日志如下。 问题原因 SecureRandom这个jre的工具类的问题。 解决方案：在Tomcat环境中解决。 可以通过配置JRE使用非阻塞的Entropy Source。 在catalina.sh文件中加入如下内容 -Djava.security.egd=file:/dev/./urandom 加入后重启Tomcat，查看Tomcat服务启动日志，启动耗时下降。 在JVM环境中解决 打开 $JAVA_PATH/jre/lib/security/java.security这个文件。 在文件中找到如下内容。 securerandom.source=file:/dev/urandom 将内容替换成如下内容 securerandom.source=file:/dev/./urandom","categories":[{"name":"Tomcat","slug":"Tomcat","permalink":"http://www.ithelei.com/categories/Tomcat/"}],"tags":[{"name":"Tomcat","slug":"Tomcat","permalink":"http://www.ithelei.com/tags/Tomcat/"}]},{"title":"Linux性能优化实战","slug":"Linux性能优化实战","date":"2019-08-31T11:33:45.000Z","updated":"2019-08-31T13:35:58.695Z","comments":true,"path":"2019/08/31/Linux性能优化实战/","link":"","permalink":"http://www.ithelei.com/2019/08/31/Linux性能优化实战/","excerpt":"","text":"如图：","categories":[{"name":"Linux","slug":"Linux","permalink":"http://www.ithelei.com/categories/Linux/"},{"name":"性能","slug":"Linux/性能","permalink":"http://www.ithelei.com/categories/Linux/性能/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.ithelei.com/tags/Linux/"},{"name":"性能","slug":"性能","permalink":"http://www.ithelei.com/tags/性能/"}]},{"title":"到底应该怎么理解（平均负载）？","slug":"到底应该怎么理解（平均负载）","date":"2019-08-30T13:27:24.000Z","updated":"2019-08-31T14:08:59.255Z","comments":true,"path":"2019/08/30/到底应该怎么理解（平均负载）/","link":"","permalink":"http://www.ithelei.com/2019/08/30/到底应该怎么理解（平均负载）/","excerpt":"","text":"每次发现系统变慢时，我们通常做的第一件事，就是执行 top 或者 uptime 命令，来了解系统的负载情况。比如像下面这样，我在命令行里输入了 uptime 命令，系统也随即给出了结果。 ` $uptime 02:34:03 up 2 days, 20:14, 1 user, load average: 0.63, 0.83, 0.88 `但我想问的是，你真的知道这里每列输出的含义吗？ 我相信你对前面的几列比较熟悉，它们分别是当前时间、系统运行时间以及正在登录用户数。 ` 02:34:03 // 当前时间 up 2 days, 20:14 // 系统运行时间 1 user // 正在登录用户数 `而最后三个数字呢，依次则是过去 1 分钟、5 分钟、15 分钟的平均负载（Load Average）。 平均负载？这个词对很多人来说，可能既熟悉又陌生，我们每天的工作中，也都会提到这个词，但你真正理解它背后的含义吗？ 如何观测和理解这个最常见、也是最重要的系统指标。 平均负载不就是单位时间内的 CPU 使用率吗？上面的 0.63，就代表 CPU 使用率是 63%。其实并不是这样，如果你方便的话，可以通过执行 man uptime 命令，来了解平均负载的详细解释。 简单来说，平均负载是指单位时间内，系统处于可运行状态 和不可中断状态的平均进程数，也就是平均活跃进程数 ，它和 CPU 使用率并没有直接关系。这里我先解释下，可运行状态和不可中断状态这俩词儿。 所谓可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。 不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。 比如，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程或者中断打断的，这个时候的进程就处于不可中断状态。如果此时的进程被打断了，就容易出现磁盘数据与进程数据不一致的问题。 所以，不可中断状态实际上是系统对进程和硬件设备的一种保护机制 因此，你可以简单理解为，平均负载其实就是平均活跃进程数。平均活跃进程数，直观上的理解就是单位时间内的活跃进程数，但它实际上是活跃进程数的指数衰减平均值。这个“指数衰减平均”的详细含义你不用计较，这只是系统的一种更快速的计算方式，你把它直接当成活跃进程数的平均值也没问题。 既然平均的是活跃进程数，那么最理想的，就是每个 CPU 上都刚好运行着一个进程，这样每个 CPU 都得到了充分利用。比如当平均负载为 2 时，意味着什么呢？ 在只有 2 个 CPU 的系统上，意味着所有的 CPU 都刚好被完全占用。 在 4 个 CPU 的系统上，意味着 CPU 有 50% 的空闲。 而在只有 1 个 CPU 的系统中，则意味着有一半的进程竞争不到 CPU。 平均负载为多少时合理说完了什么是平均负载，现在我们再回到最开始的例子，在 uptime 命令的结果里，那三个时间段的平均负载数，多大的时候能说明系统负载高？或是多小的时候就能说明系统负载很低呢？ 我们知道，平均负载最理想的情况是等于 CPU 个数。所以在评判平均负载时，首先你要知道系统有几个 CPU ，这可以通过 top 命令或者从文件 /proc/cpuinfo 中读取，比如： ` # 关于 grep 和 wc 的用法请查询它们的手册或者网络搜索 $ grep &apos;model name&apos; /proc/cpuinfo | wc -l 2 `有了 CPU 个数，我们就可以判断出，当平均负载比 CPU 个数还大的时候，系统已经出现了过载。 不过，且慢，新的问题又来了。我们在例子中可以看到，平均负载有三个数值，到底该参考哪一个呢？ 实际上，都要看。三个不同时间间隔的平均值，其实给我们提供了，分析系统负载趋势的数据来源，让我们能更全面、更立体地理解目前的负载状况。 打个比方，就像初秋时北京的天气，如果只看中午的温度，你可能以为还在 7 月份的大夏天呢。但如果你结合了早上、中午、晚上三个时间点的温度来看，基本就可以全方位了解这一天的天气情况了。 同样的，前面说到的 CPU 的三个负载时间段也是这个道理。 如果 1 分钟、5 分钟、15 分钟的三个值基本相同，或者相差不大，那就说明系统负载很平稳。 但如果 1 分钟的值远小于 15 分钟的值，就说明系统最近 1 分钟的负载在减少，而过去 15 分钟内却有很大的负载。 反过来，如果 1 分钟的值远大于 15 分钟的值，就说明最近 1 分钟的负载在增加，这种增加有可能只是临时性的，也有可能还会持续增加下去，所以就需要持续观察。一旦 1 分钟的平均负载接近或超过了 CPU 的个数，就意味着系统正在发生过载的问题，这时就得分析调查是哪里导致的问题，并要想办法优化了。 这里我再举个例子，假设我们在一个单 CPU 系统上看到平均负载为 1.73，0.60，7.98，那么说明在过去 1 分钟内，系统有 73% 的超载，而在 15 分钟内，有 698% 的超载，从整体趋势来看，系统的负载在降低。 那么，在实际生产环境中，平均负载多高时，需要我们重点关注呢？ 在我看来，当平均负载高于 CPU 数量 70% 的时候 ，你就应该分析排查负载高的问题了。一旦负载过高，就可能导致进程响应变慢，进而影响服务的正常功能。 但 70% 这个数字并不是绝对的，最推荐的方法，还是把系统的平均负载监控起来，然后根据更多的历史数据，判断负载的变化趋势。当发现负载有明显升高趋势时，比如说负载翻倍了，你再去做分析和调查。 平均负载与 CPU 使用率现实工作中，我们经常容易把平均负载和 CPU 使用率混淆，所以在这里，我也做一个区分。 可能你会疑惑，既然平均负载代表的是活跃进程数，那平均负载高了，不就意味着 CPU 使用率高吗？ 我们还是要回到平均负载的含义上来，平均负载是指单位时间内，处于可运行状态和不可中断状态的进程数。所以，它不仅包括了 而 CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。比如： 而 CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。比如： I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高； 大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。 平均负载案例分析下面，我们以三个示例分别来看这三种情况，并用 iostat、mpstat、pidstat 等工具，找出平均负载升高的根源。 因为案例分析都是基于机器上的操作，所以不要只是听听、看看就够了，最好还是跟着我实际操作一下。 你的准备下面的案例都是基于 Ubuntu 18.04，当然，同样适用于其他 Linux 系统。我使用的案例环境如下所示。 机器配置：2 CPU，8GB 内存。 预先安装 stress 和 sysstat 包，如 apt install stress sysstat。 在这里，我先简单介绍一下 stress 和 sysstat。 stress 是一个 Linux 系统压力测试工具，这里我们用作异常进程模拟平均负载升高的场景。 而 sysstat 包含了常用的 Linux 性能工具，用来监控和分析系统的性能。我们的案例会用到这个包的两个命令 mpstat 和 pidstat。 mpstat 是一个常用的多核 CPU 性能分析工具，用来实时查看每个 CPU 的性能指标，以及所有 CPU 的平均指标。 pidstat 是一个常用的进程性能分析工具，用来实时查看进程的 CPU、内存、I/O 以及上下文切换等性能指标。 此外，每个场景都需要你开三个终端，登录到同一台 Linux 机器中。 实验之前，你先做好上面的准备。如果包的安装有问题，可以先在 Google 一下自行解决，如果还是解决不了，再来留言区找我，这事儿应该不难。 另外要注意，下面的所有命令，我们都是默认以 root 用户运行。所以，如果你是用普通用户登陆的系统，一定要先运行 sudo su root 命令切换到 root 用户。 如果上面的要求都已经完成了，你可以先用 uptime 命令，看一下测试前的平均负载情况： `$uptame ..., load average: 0.11, 0.15, 0.09 `场景一：CPU 密集型进程首先，我们在第一个终端运行 stress 命令，模拟一个 CPU 使用率 100% 的场景： ` $ stress --cpu 1 --timeout 600 `接着，在第二个终端运行 uptime 查看平均负载的变化情况： ` # -d 参数表示高亮显示变化的区域 $ watch -d uptime ..., load average: 1.00, 0.75, 0.39 `最后，在第三个终端运行 mpstat 查看 CPU 使用率的变化情况： `# -P ALL 表示监控所有 CPU，后面数字 5 表示间隔 5 秒后输出一组数据 $ mpstat -P ALL 5 Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU) 13:30:06 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 13:30:11 all 50.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 49.95 13:30:11 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 13:30:11 1 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 `从终端二中可以看到，1 分钟的平均负载会慢慢增加到 1.00，而从终端三中还可以看到，正好有一个 CPU 的使用率为 100%，但它的 iowait 只有 0。这说明，平均负载的升高正是由于 CPU 使用率为 100% 。 那么，到底是哪个进程导致了 CPU 使用率为 100% 呢？你可以使用 pidstat 来查询： ` # 间隔 5 秒后输出一组数据 $ pidstat -u 5 1 13:37:07 UID PID %usr %system %guest %wait %CPU CPU Command 13:37:12 0 2962 100.00 0.00 0.00 0.00 100.00 1 stress `从这里可以明显看到，stress 进程的 CPU 使用率为 100%。 场景二：I/O 密集型进程首先还是运行 stress 命令，但这次模拟 I/O 压力，即不停地执行 sync： `$stress -i 1 --timeout 600 `还是在第二个终端运行 uptime 查看平均负载的变化情况： `$watch -d uptime ..., load average: 1.06, 0.58, 0.37 `然后，第三个终端运行 mpstat 查看 CPU 使用率的变化情况： `# 显示所有 CPU 的指标，并在间隔 5 秒输出一组数据 $ mpstat -P ALL 5 1 Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU) 13:41:28 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 13:41:33 all 0.21 0.00 12.07 32.67 0.00 0.21 0.00 0.00 0.00 54.84 13:41:33 0 0.43 0.00 23.87 67.53 0.00 0.43 0.00 0.00 0.00 7.74 13:41:33 1 0.00 0.00 0.81 0.20 0.00 0.00 0.00 0.00 0.00 98.99 `从这里可以看到，1 分钟的平均负载会慢慢增加到 1.06，其中一个 CPU 的系统 CPU 使用率升高到了 23.87，而 iowait 高达 67.53%。这说明，平均负载的升高是由于 iowait 的升高。 那么到底是哪个进程，导致 iowait 这么高呢？我们还是用 pidstat 来查询： `# 间隔 5 秒后输出一组数据，-u 表示 CPU 指标 $ pidstat -u 5 1 Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU) 13:42:08 UID PID %usr %system %guest %wait %CPU CPU Command 13:42:13 0 104 0.00 3.39 0.00 0.00 3.39 1 kworker/1:1H 13:42:13 0 109 0.00 0.40 0.00 0.00 0.40 0 kworker/0:1H 13:42:13 0 2997 2.00 35.53 0.00 3.99 37.52 1 stress 13:42:13 0 3057 0.00 0.40 0.00 0.00 0.40 0 pidstat `可以发现，还是 stress 进程导致的。 场景三：大量进程的场景当系统中运行进程超出 CPU 运行能力时，就会出现等待 CPU 的进程。 比如，我们还是使用 stress，但这次模拟的是 8 个进程： `$stress -c 8 --timeout 600 `由于系统只有 2 个 CPU，明显比 8 个进程要少得多，因而，系统的 CPU 处于严重过载状态，平均负载高达 7.97： `$uptime ..., load average: 7.97, 5.93, 3.02 `接着再运行 pidstat 来看一下进程的情况： `# 间隔 5 秒后输出一组数据 $ pidstat -u 5 1 14:23:25 UID PID %usr %system %guest %wait %CPU CPU Command 14:23:30 0 3190 25.00 0.00 0.00 74.80 25.00 0 stress 14:23:30 0 3191 25.00 0.00 0.00 75.20 25.00 0 stress 14:23:30 0 3192 25.00 0.00 0.00 74.80 25.00 1 stress 14:23:30 0 3193 25.00 0.00 0.00 75.00 25.00 1 stress 14:23:30 0 3194 24.80 0.00 0.00 74.60 24.80 0 stress 14:23:30 0 3195 24.80 0.00 0.00 75.00 24.80 0 stress 14:23:30 0 3196 24.80 0.00 0.00 74.60 24.80 1 stress 14:23:30 0 3197 24.80 0.00 0.00 74.80 24.80 1 stress 14:23:30 0 3200 0.00 0.20 0.00 0.20 0.20 0 pidstat `可以看出，8 个进程在争抢 2 个 CPU，每个进程等待 CPU 的时间（也就是代码块中的 %wait 列）高达 75%。这些超出 CPU 计算能力的进程，最终导致 CPU 过载。 小结分析完这三个案例，我再来归纳一下平均负载的理解 平均负载提供了一个快速查看系统整体性能的手段，反映了整体的负载情况。但只看平均负载本身，我们并不能直接发现，到底是哪里出现了瓶颈。所以，在理解平均负载时，也要注意： 平均负载高有可能是 CPU 密集型进程导致的； 平均负载高并不一定代表 CPU 使用率高，还有可能是 I/O 更繁忙了； 当发现负载高的时候，你可以使用 mpstat、pidstat 等工具，辅助分析负载的来源。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://www.ithelei.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.ithelei.com/tags/Linux/"}]},{"title":"百度云智峰会","slug":"百度云智峰会","date":"2019-08-29T01:20:45.000Z","updated":"2019-09-01T12:36:57.939Z","comments":true,"path":"2019/08/29/百度云智峰会/","link":"","permalink":"http://www.ithelei.com/2019/08/29/百度云智峰会/","excerpt":"","text":"百度云智峰会，科技为更好。","categories":[{"name":"峰会","slug":"峰会","permalink":"http://www.ithelei.com/categories/峰会/"}],"tags":[{"name":"峰会","slug":"峰会","permalink":"http://www.ithelei.com/tags/峰会/"}]},{"title":"百度云智峰会（实战环节）","slug":"百度云智峰会 （实战）本","date":"2019-08-29T01:20:45.000Z","updated":"2019-09-01T12:50:52.310Z","comments":true,"path":"2019/08/29/百度云智峰会 （实战）本/","link":"","permalink":"http://www.ithelei.com/2019/08/29/百度云智峰会 （实战）本/","excerpt":"","text":"百度云智峰会，上机实战环节。在这里由于表现突出，得了一个自己特别喜欢小奖品。","categories":[{"name":"峰会","slug":"峰会","permalink":"http://www.ithelei.com/categories/峰会/"}],"tags":[{"name":"峰会","slug":"峰会","permalink":"http://www.ithelei.com/tags/峰会/"}]},{"title":"MySQL优化(1)","slug":"MySQL优化(1)","date":"2019-08-28T14:39:45.000Z","updated":"2019-08-28T13:21:45.209Z","comments":true,"path":"2019/08/28/MySQL优化(1)/","link":"","permalink":"http://www.ithelei.com/2019/08/28/MySQL优化(1)/","excerpt":"","text":"MySQL查询优化应注意的问题 对查询进行优化，应尽量避免全表扫描，首先应考虑在where及order by 涉及的列上建立索引。 应尽量避免在where子句中使用！=或 &lt;&gt; 操作符，否则引擎将放弃索引而进行全表扫描。 应尽量避免在where字句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描，如select id from t where num is null 可以在num上设置默认值为0，确保表中num列没有null值，如select id from t where num = 0。 应尽量避免在where子句中使用or来连接条件，否则将导致引擎放弃使用索引而进行全表扫描。如下面的语句：select id from t where num =10 or num =20,可以改成下面的语句：select id from t where num =10 union all select id from t where num =20 下面的查询也将导致全表扫描： select id from t where name like &#39;%abc%&#39; in 和 not in也要慎用，否则会导致全表扫描。如下面的语句： select id from t where num in (1,2,3) 对于连续的数值，能用between 就不要用 in 了。 select id from t where num between 1 and 3 如果在where子句中使用参数，也会导致全表扫描。因为sql只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择选择推迟到运行时；他必须在编译时进行选择。然而。如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择输入项。如下面的语句将进行全表扫描。 select id from t where num=@num 可以改为强制查询使用索引，如下语句： select id from t with (index(索引名)) where num =@num 应尽量避免在where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描，如下面的语句： select id from t where num/2=100应改为 select id from t where num=100*2 应尽量避免在where子句中 对字段进行函数操作，这将导致殷勤放弃使用索引而进行全表扫描，如下面的语句： select id from t where SUBSTRING(name,1,3)=&#39;abc&#39; –name以abc开头的id select id from t where DATEDIFF(day,createdate,&#39;2019-08-28&#39;)=0 –2019-08-28生成的id 。 应改为如下面的语句： `select id from ｔ where nane like &apos;abc%&apos;` `select id from t where createdate &gt; = &apos;2019-08-28&apos; and createdate &lt; &apos;2019-08-29&apos;` 不要在where 子句中的 “=” 左边进行函数、算数运算或其他表达式运算。否则系统将可能无法正确使用索引。 在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 很多时候用exists代替in是一个好的选择。 select num from a where num in （select num from b） 用下面的语句替换。 `select num from a where exists （select 1 from b where num=a.num）`","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://www.ithelei.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://www.ithelei.com/tags/MySQL/"}]},{"title":"MySQL优化(2)","slug":"MySQL优化(2)","date":"2019-08-28T13:36:45.000Z","updated":"2019-08-28T14:03:47.468Z","comments":true,"path":"2019/08/28/MySQL优化(2)/","link":"","permalink":"http://www.ithelei.com/2019/08/28/MySQL优化(2)/","excerpt":"","text":"继MySQL优化(1) 并不是所有索引对查询都有效，sql语句是根军表中数据来进行优化的，当索引列有大量数据重复时，sql查询可能不会去利用索引，如某表中有字段sex,male,female几乎各一半，那么及时在sex上建了索引也对查询效率起不了作用。 索引并不是越多越好，索引固然可以提高相应的select的效率，但同时也降低了insert和update的效率，因为insert和update时有可能会重建索引，所以怎样建索引需要慎重考虑，是具体情况而定。一个表的索引不要超过5个。若太多则时，考虑那些不常使用列上的索引是否有必要。 应尽可能地避免更新 lustered素引数据列，因为 clustered索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要烦繁更新 clustered索引数据列，那么需要考虑是否将该索引建为 clustered索引。 尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每个字符，而对于数字型而言只需要比较一次就够了。 尽可能地使用 VARCHAR/NVARCHAR代替 CHAR/NCHAR，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 任何地方都不要使用 SELECT＊ FROM t，用具体的字段列表代替“＊”，不要返回用不到的任何字段。 尽量使用表变量来代替临时表。如果表变量包含大量数据，应注意索引非常有限(只有主键索引)。 避免频繁创建和删除临时表，以减少系统表资源的消耗 临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用,大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 在新建临时表时，如果一次性插入数据量很大，那么可以使用 SELECT into代替 CREATEtable，避免造成大量log，以提高速度:如果数据量不大，为了缓和系统表的资源，应先CREATE table，然后 INSERT 如果使用到了临时表，在存储过程的最后务必将所有的临时表显式刷除，先 truncate tabl然后 DROP table，这样可以避免系统表的较长时间锁定。 尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应考虑改写 使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效 与临时表一样，游标并不是不可使用。对小型数据集使用FAST_ FORWARD游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的历程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好 在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON，在结束时设置SETNOCOUNT OFF。无须在执行存储过程和触发器的每个语句后向客户端发送 DONE INPROC消息。 尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理 尽量避免大事务操作，提高系统并发能力","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://www.ithelei.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://www.ithelei.com/tags/MySQL/"}]},{"title":"MySQL规范","slug":"MySQL规范","date":"2019-08-27T13:45:00.000Z","updated":"2019-08-27T14:16:51.451Z","comments":true,"path":"2019/08/27/MySQL规范/","link":"","permalink":"http://www.ithelei.com/2019/08/27/MySQL规范/","excerpt":"","text":"一、基础规范 表存储引擎必须使用InnoDB 表字符集默认使用utf8，必要时候使用utf8mb4 解读： （1）通用，无乱码风险，汉字3字节，英文1字节。 （2）utf8mb4是utf8的超集，有存储4字节例如表情符号时，使用它。 禁止使用存储过程，视图，触发器，Event。 解读： （1）对数据库性能影响较大，互联网业务，能让站点层和服务层干的事情，不要交到数据库层。 （2）调试，排错，迁移都比较困难，扩展性较差。 禁止在数据库中存储大文件，例如照片，可以将大文件存储在对象存储系统，数据库中存储路径。 禁止在线上环境做数据库压力测试。 测试，开发，线上数据库环境必须隔离。 二、命名规范 库名，表名，列名必须用小写，采用下划线分隔。 解读：abc，Abc，ABC都是不允许的。 库名，表名，列名必须见名知义，长度不要超过32字符。 解读：tmp，wujun谁TM知道这些库是干嘛的。 库备份必须以bak为前缀，以日期为后缀。 从库必须以-s为后缀。 备库必须以-ss为后缀。 三、表设计规范 单实例表个数必须控制在2000个以内。 单表分表个数必须控制在1024个以内。 必须有主键，推荐使用UNSIGNED整数为主键。 潜在坑：删除无主键的表，如果是row模式的主从架构，从库会挂住 禁止使用外键，如果要保证完整性，应由应用程式实现。 解读：外键使得表之间相互耦合，影响update/delete等SQL性能，有可能造成死锁，高并发情况下容易成为数据库瓶颈 建议将大字段，访问频度低的字段拆分到单独的表中存储，分离冷热数据。 四、列设计规范 根据业务区分使用tinyint/int/bigint，分别会占用1/4/8字节。 根据业务区分使用char/varchar。 解读： （1）字段长度固定，或者长度近似的业务场景，适合使用char，能够减少碎片，查询性能高 （2）字段长度相差较大，或者更新较少的业务场景，适合使用varchar，能够减少空间。 根据业务区分使用datetime/timestamp。 解读：前者占用5个字节，后者占用4个字节，存储年使用YEAR，存储日期使用DATE，存储时间使用datetime。 必须把字段定义为NOT NULL并设默认值。 解读： （1）NULL的列使用索引，索引统计，值都更加复杂，MySQL更难优化 （2）NULL需要更多的存储空间 （3）NULL只能采用IS NULL或者IS NOT NULL，而在=/!=/in/not in时有大坑 使用INT UNSIGNED存储IPv4，不要用char(15)。 使用varchar(20)存储手机号，不要使用整数。 解读： （1）牵扯到国家代号，可能出现+/-/()等字符，例如+86 （2）手机号不会用来做数学运算 （3）varchar可以模糊查询，例如like ‘138%’ 使用TINYINT来代替ENUM。 解读：ENUM增加新值要进行DDL操作。 五、索引规范 唯一索引使用uniq_[字段名]来命名。 非唯一索引使用idx_[字段名]来命名。 单张表索引数量建议控制在5个以内。 解读： （1）互联网高并发业务，太多索引会影响写性能。 （2）生成执行计划时，如果索引太多，会降低性能，并可能导致MySQL选择不到最优索引。 （3）异常复杂的查询需求，可以选择ES等更为适合的方式存储。 组合索引字段数不建议超过5个。 解读：如果5个字段还不能极大缩小row范围，八成是设计有问题 不建议在频繁更新的字段上建立索引。 非必要不要进行JOIN查询，如果要进行JOIN查询，被JOIN的字段必须类型相同，并建立索引。 解读：因为JOIN字段类型不一致，而导致全表扫描。 理解组合索引最左前缀原则，避免重复建设索引，如果建立了(a,b,c)，相当于建立了(a), (a,b), (a,b,c)。 六、SQL规范 禁止使用select *，只获取必要字段。 解读： （1）select *会增加cpu/io/内存/带宽的消耗。 （2）指定字段能有效利用索引覆盖。 （3）指定字段查询，在表结构变更时，能保证对应用程序无影响。 insert必须指定字段，禁止使用insert into T values()。 解读：指定字段插入，在表结构变更时，能保证对应用程序无影响。 隐式类型转换会使索引失效，导致全表扫描。 禁止在where条件列使用函数或者表达式。 解读：导致不能命中索引，全表扫描 禁止负向查询以及%开头的模糊查询。 解读：导致不能命中索引，全表扫描。 禁止大表JOIN和子查询。 同一个字段上的OR必须改写问IN，IN的值必须少于50个。 应用程序必须捕获SQL异常。 解读：方便定位线上问题","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://www.ithelei.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://www.ithelei.com/tags/MySQL/"}]},{"title":"阿里云标准-Apache Tomcat 安全基线检查","slug":"阿里云标准-Apache Tomcat 安全基线检查","date":"2019-08-27T13:27:24.000Z","updated":"2019-08-27T13:36:34.131Z","comments":true,"path":"2019/08/27/阿里云标准-Apache Tomcat 安全基线检查/","link":"","permalink":"http://www.ithelei.com/2019/08/27/阿里云标准-Apache Tomcat 安全基线检查/","excerpt":"","text":"Tomcat进程运行权限检测 | 访问控制描述 在运行Internet服务时，最好尽可能避免使用root用户运行，降低攻击者拿到服务器控制权限的机会。 加固建议 创建低权限的账号运行Tomcat 操作时建议做好记录或备份 Tomcat目录权限检测 | 访问控制描述 在运行Tomcat服务时，避免使用root用户运行，tomcat目录(catalina.home、 catalina.base目录)所有者应改为非root的运行用户 加固建议 使用chown -R &lt;Tomcat启动用户所属组&gt;:&lt;Tomcat启动用户&gt; &lt;Tomcat目录&gt;修改tomcat目录文件所有者，如chown -R tomcat:tomcat /usr/local/tomcat 操作时建议做好记录或备份 限制服务器平台信息泄漏 | 服务配置描述 限制服务器平台信息泄漏会使攻击者更难确定哪些漏洞会影响服务器平台。 加固建议 1、进入Tomcat安装主目录的lib目录下，比如 cd /usr/local/tomcat7/lib 2、执行：jar xf catalina.jar org/apache/catalina/util/ServerInfo.properties，修改文件ServerInfo.properties中的server.info和server.number的值，如分别改为：Apache/11.0.92、11.0.92.0 3、执行：jar uf catalina.jar org/apache/catalina/util/ServerInfo.properties 4、重启Tomcat服务 操作时建议做好记录或备份 禁止自动部署 | 服务配置描述 配置自动部署，容易被部署恶意或未经测试的应用程序，应将其禁用 加固建议 修改Tomcat 跟目录下的配置文件conf/server.xml，将host节点的autoDeploy属性设置为“false”，如果host的deployOnStartup属性(如没有deployOnStartup配置可以忽略)为“true”，则也将其更改为“false” 操作时建议做好记录或备份 禁止显示异常调试信息 | 服务配置描述 当请求处理期间发生运行时错误时，ApacheTomcat将向请求者显示调试信息。建议不要向请求者提供此类调试信息。 加固建议 在Tomcat根目录下的conf/web.xml文件里面的web-app添加子节点：java.lang.Throwable/error.jsp，在webapps目录下创建error.jsp，定义自定义错误信息 操作时建议做好记录或备份 开启日志记录 | 安全审计描述 Tomcat需要保存输出日志，以便于排除错误和发生安全事件时，进行分析和定位 加固建议 1、修改Tomcat根目录下的conf/server.xml文件。 2、取消Host节点下Valve节点的注释(如没有则添加)。 3、重新启动Tomcat 操作时建议做好记录或备份 禁止Tomcat显示目录文件列表 | 服务配置描述 Tomcat允许显示目录文件列表会引发目录遍历漏洞 加固建议 修改Tomcat 跟目录下的配置文件conf/web.xml，将listings的值设置为false。 listings false 操作时建议做好记录或备份 删除项目无关文件和目录 | 访问控制描述 Tomcat安装提供了示例应用程序、文档和其他可能不用于生产程序及目录，存在极大安全风险，建议移除 加固建议 请删除Tomcat示例程序和目录、管理控制台等，即从Tomcat根目录的webapps目录，移出或删除docs、examples、host-manager、manager目录。 操作时建议做好记录或备份 避免为tomcat配置manager-gui弱口令 | 访问控制描述 tomcat-manger是Tomcat提供的web应用热部署功能，该功能具有较高权限，会直接控制Tomcat应用，应尽量避免使用此功能。如有特殊需求，请务必确保为该功能配置了强口令 加固建议 编辑Tomcat根目录下的配置文件conf/tomcat-user.xml，修改user节点的password属性值为复杂密码, 密码应符合复杂性要求： 1、长度8位以上 2、包含以下四类字符中的三类字符:英文大写字母(A 到 Z)英文小写字母(a 到 z)10 个基本数字(0 到 9)非字母字符(例如 !、$、#、%、@、^、&amp;) 3、避免使用已公开的弱密码，如：abcd.1234 、admin@123等操作时建议做好记录或备份","categories":[{"name":"安全","slug":"安全","permalink":"http://www.ithelei.com/categories/安全/"}],"tags":[{"name":"基线","slug":"基线","permalink":"http://www.ithelei.com/tags/基线/"},{"name":"Apache Tomcat","slug":"Apache-Tomcat","permalink":"http://www.ithelei.com/tags/Apache-Tomcat/"}]},{"title":"CentOS Linux 7安全基线检查","slug":"CentOS Linux 7安全基线检查","date":"2019-08-26T14:25:24.000Z","updated":"2019-08-27T13:20:42.155Z","comments":true,"path":"2019/08/26/CentOS Linux 7安全基线检查/","link":"","permalink":"http://www.ithelei.com/2019/08/26/CentOS Linux 7安全基线检查/","excerpt":"","text":"设置用户权限配置文件的权限 | 文件权限描述 设置用户权限配置文件的权限 加固建议 执行以下5条命令 chown root:root /etc/passwd /etc/shadow /etc/group /etc/gshadow chmod 0644 /etc/group chmod 0644 /etc/passwd chmod 0400 /etc/shadow chmod 0400 /etc/gshadow 操作时建议做好记录或备份 确保SSH LogLevel设置为INFO | 服务配置描述 确保SSH LogLevel设置为INFO,记录登录和注销活动 加固建议 编辑 /etc/ssh/sshd_config 文件以按如下方式设置参数(取消注释): LogLevel INFO 操作时建议做好记录或备份 设置SSH空闲超时退出时间 | 服务配置描述 设置SSH空闲超时退出时间,可降低未授权用户访问其他用户ssh会话的风险 加固建议 编辑/etc/ssh/sshd_config，将ClientAliveInterval 设置为300到900，即5-15分钟，将ClientAliveCountMax设置为0。 ClientAliveInterval 900 ClientAliveCountMax 0 操作时建议做好记录或备份 SSHD强制使用V2安全协议 | 服务配置描述 SSHD强制使用V2安全协议 加固建议 编辑 /etc/ssh/sshd_config 文件以按如下方式设置参数： Protocol 2 操作时建议做好记录或备份 确保SSH MaxAuthTries设置为3到6之间 | 服务配置描述 设置较低的Max AuthTrimes参数将降低SSH服务器被暴力攻击成功的风险。 加固建议 在/etc/ssh/sshd_config中取消MaxAuthTries注释符号#，设置最大密码尝试失败次数3-6，建议为4： MaxAuthTries 4 操作时建议做好记录或备份 设置密码修改最小间隔时间 | 身份鉴别描述 设置密码修改最小间隔时间，限制密码更改过于频繁 加固建议 在 /etc/login.defs 中将 PASS_MIN_DAYS 参数设置为7-14之间,建议为7： PASS_MIN_DAYS 7 需同时执行命令为root用户设置： chage –mindays 7 root 操作时建议做好记录或备份 设置密码失效时间 | 身份鉴别描述 设置密码失效时间，强制定期修改密码，减少密码被泄漏和猜测风险，使用非密码登陆方式(如密钥对)请忽略此项。 加固建议 使用非密码登陆方式如密钥对，请忽略此项。在 /etc/login.defs 中将 PASS_MAX_DAYS 参数设置为 60-180之间，如 PASS_MAX_DAYS 90。需同时执行命令设置root密码失效时间： chage –maxdays 90 root。 操作时建议做好记录或备份 禁止SSH空密码用户登录 | 服务配置描述 禁止SSH空密码用户登录 加固建议 在/etc/ssh/sshd_config中取消PermitEmptyPasswords no注释符号# 操作时建议做好记录或备份 确保root是唯一的UID为0的帐户 | 身份鉴别描述 除root以外其他UID为0的用户都应该删除，或者为其分配新的UID 加固建议 除root以外其他UID为0的用户(查看命令cat /etc/passwd | awk -F: ‘($3 == 0) { print $1 }’|grep -v ‘^root$’ )都应该删除，或者为其分配新的UID 操作时建议做好记录或备份","categories":[{"name":"安全","slug":"安全","permalink":"http://www.ithelei.com/categories/安全/"}],"tags":[{"name":"CentOS Linux 7","slug":"CentOS-Linux-7","permalink":"http://www.ithelei.com/tags/CentOS-Linux-7/"},{"name":"基线","slug":"基线","permalink":"http://www.ithelei.com/tags/基线/"}]},{"title":"QingCloud峰会","slug":"青云峰会","date":"2019-06-03T01:20:45.000Z","updated":"2019-09-01T12:19:03.426Z","comments":true,"path":"2019/06/03/青云峰会/","link":"","permalink":"http://www.ithelei.com/2019/06/03/青云峰会/","excerpt":"","text":"参加QingCloud峰会，了解互联网最前沿技术科技为更好","categories":[{"name":"峰会","slug":"峰会","permalink":"http://www.ithelei.com/categories/峰会/"}],"tags":[{"name":"峰会","slug":"峰会","permalink":"http://www.ithelei.com/tags/峰会/"}]},{"title":"参加阿里云峰会","slug":"阿里云峰会","date":"2019-03-01T01:20:45.000Z","updated":"2019-09-01T12:14:15.602Z","comments":true,"path":"2019/03/01/阿里云峰会/","link":"","permalink":"http://www.ithelei.com/2019/03/01/阿里云峰会/","excerpt":"","text":"参加阿里云峰会，了解互联网最前沿技术十年再出发 科技为更好","categories":[{"name":"峰会","slug":"峰会","permalink":"http://www.ithelei.com/categories/峰会/"}],"tags":[{"name":"峰会","slug":"峰会","permalink":"http://www.ithelei.com/tags/峰会/"}]}]}